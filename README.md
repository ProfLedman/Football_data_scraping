# FBref Big-5 Fixtures Scraper

A production-quality web application that scrapes Football Reference (FBref) for today's Big-5 league fixtures and generates CSV/XLSX reports with team and player data.

## Features

- **Live Fixture Discovery**: Automatically discovers today's fixtures from Premier League, La Liga, Serie A, Bundesliga, and Ligue 1
- **One-Click Export**: Generate reports (CSV/XLSX) for any fixture
- **Real-Time Progress**: Live progress bar with detailed status updates
- **Anti-Bot Protection**: handling of Cloudflare challenges and decoy pages
- **Complete Data Extraction**: All team tables + individual player stats 
- **Polite Scraping**: Configurable delays and rate limiting to respect FBref servers

## Quick Start

### Local Development
```bash
# Clone and setup
git clone <repo-url>
cd fbref-scraper
pip install -r requirements.txt

# Run the application
python app.py
# Open http://localhost:8000
```

### Docker Deployment
```bash
# Build and run
docker build -t fbref-scraper .
docker run --rm -p 8000:8000 fbref-scraper
```

### Cloud Deployment (Heroku/Render)
```bash
# For Heroku
heroku create your-app-name
git push heroku main

# For Render - connect your GitHub repo and deploy
```

## Configuration

Edit `config.yaml` to customize:
```yaml
scraping:
  base_delay_min: 5  # Min delay between requests (seconds)
  base_delay_max: 10 # Max delay between requests
  max_retries: 3
  timeout: 30
  
user_agents:
  - "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
  # Add more user agents as needed

proxy:
  enabled: false
  urls: []  # Add proxy URLs if needed
```

## API Endpoints

- `GET /` - Main dashboard
- `GET /api/fixtures` - Get today's Big-5 fixtures
- `POST /api/generate/{fixture_id}` - Generate report for fixture
- `GET /api/progress/{task_id}` - Get generation progress (SSE)
- `GET /api/download/{task_id}` - Download generated report

## Output Structure

Generated XLSX files contain:
- **Home Team Sheets**: All available team statistics tables
- **Away Team Sheets**: All available team statistics tables  
- **Player Sheets**: Individual player stats, match logs, and season splits
- **Manifest Sheet**: Summary of extracted data with timestamps and source URLs

Expected output: ~50-55 sheets of tabular data per complete match.

## Architecture

```
fbref-scraper/
├── app.py                 # FastAPI application
├── scraper/
│   ├── __init__.py
│   ├── core.py           # Main scraping engine
│   ├── fixtures.py       # Fixture discovery
│   ├── parser.py         # HTML parsing utilities
│   ├── anti_bot.py       # Anti-bot detection
│   └── export.py         # Data export utilities
├── static/
│   ├── index.html        # Frontend UI
│   ├── style.css
│   └── app.js
├── tests/
│   ├── test_parser.py    # HTML comment parsing tests
│   ├── test_fixtures.py  # Fixture discovery tests
│   └── sample_data/      # Sample HTML files
├── config.yaml           # Configuration
├── requirements.txt      # Python dependencies
├── Dockerfile           # Container setup
└── README.md
```

## Testing

```bash
# Run all tests
pytest tests/

# Test HTML comment table parsing
pytest tests/test_parser.py::test_comment_table_parsing

# Test fixture discovery
pytest tests/test_fixtures.py::test_big5_fixture_discovery
```

## Sample Output

See `sample_reports/` directory for example XLSX files generated by the scraper.

## Troubleshooting

### Common Issues
1. **Cloudflare Blocking**: The scraper automatically detects and retries on decoy pages
2. **Rate Limiting**: Increase delays in `config.yaml` if getting blocked
3. **Missing Data**: Some tables may be unavailable for certain matches

### Diagnostics
- Check `/api/diagnostics` for recent request logs
- Monitor console output for detailed scraping progress
- Review generated manifest sheets for data completeness

## Development

### Adding New Leagues
1. Add league URLs to `scraper/fixtures.py`
2. Update fixture discovery patterns
3. Test with sample fixtures

### Extending Data Extraction
1. Add new table selectors to `scraper/parser.py` 
2. Update export logic in `scraper/export.py`
3. Add corresponding tests

## License

MIT License - See LICENSE file for details

## Contributing

1. Fork the repository
2. Create feature branch (`git checkout -b feature/new-feature`)
3. Commit changes (`git commit -am 'Add new feature'`)
4. Push to branch (`git push origin feature/new-feature`) 
5. Create Pull Request

---

**Note**: This scraper is designed for educational and research purposes. Please respect FBref's terms of service and implement appropriate delays between requests.